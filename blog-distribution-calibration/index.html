<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="Exploring Distribution Calibration for deep learning models to achieve accurate and sharp probabilistic predictions.">
  <meta property="og:title" content="Distribution Calibration: Accurate & Sharp Uncertainties in Deep Learning"/>
  <meta property="og:description" content="A method for improving the reliability of probabilistic predictions from machine learning models through distribution recalibration."/>
  <meta property="og:url" content="https://shachideshpande.github.io/blog-distribution-calibration/"/> <meta property="og:image" content="static/image/your_banner_image.png" /> <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <meta name="twitter:title" content="Distribution Calibration: Accurate & Sharp Uncertainties">
  <meta name="twitter:description" content="Improving reliability of ML model predictions with distribution recalibration.">
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png"> <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords" content="Distribution Calibration, Deep Learning, Uncertainty Quantification, Machine Learning, Probabilistic Predictions, Recalibration, Density Estimation, ICML 2022">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Distribution Calibration</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Calibrated and Sharp Uncertainties in Deep Learning via Density Estimation</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://www.cs.cornell.edu/~kuleshov/" target="_blank">Volodymyr Kuleshov</a>,</span>
              <span class="author-block">
                <a href="https://shachideshpande.github.io/" target="_blank">Shachi Deshpande</a>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">ICML 2022</span>
              </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                   <span class="link-block">
                  <a href="https://proceedings.mlr.press/v162/kuleshov22a/kuleshov22a.pdf" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper (PMLR)</span>
                </a>
              </span>

              <span class="link-block">
                <a href="https://arxiv.org/abs/2112.07184" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="ai ai-arxiv"></i>
                </span>
                <span>arXiv</span>
              </a>
            </span>

            <span class="link-block">
              <a href="https://github.com/shachideshpande/DistCal" target="_blank" class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                <i class="fab fa-github"></i>
              </span>
              <span>Code</span>
            </a>
          </span>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>
</section>

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Accurate probabilistic predictions can be characterized by two properties calibration and sharpness. However, standard maximum likelihood training yields models that are poorly calibrated and thus inaccurate a 90% confidence interval typically does not contain the true outcome 90% of the time. This paper argues that calibration is important in practice and is easy to maintain by performing low-dimensional density estimation. We introduce a simple training procedure based on recalibration that yields calibrated models without sacrificing overall performance; unlike previous approaches, ours ensures the most general property of distribution calibration and applies to any model, including neural networks. We formally prove the correctness of our procedure assuming that we can estimate densities in low dimensions and we establish uniform convergence bounds. Our results yield empirical performance improvements on linear and deep Bayesian models and suggest that calibration should be increasingly leveraged across machine learning.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">The Essence of Distribution Recalibration</h2>
        <div class="content has-text-justified">
          <p>
            The predictive uncertainty of machine learning models, particularly deep neural networks, is often miscalibrated. For instance, a 90% prediction interval from a standard maximum likelihood-trained model may not contain the true outcome with the claimed 90% frequency. This discrepancy between nominal and empirical coverage poses risks in safety-critical applications where reliable uncertainty quantification is paramount.
          </p>
          <p>
            Effective probabilistic forecasting necessitates two fundamental properties: <strong>calibration</strong>, ensuring that predicted probabilities align with observed outcome frequencies, and <strong>sharpness</strong>, indicating the concentration of predictive distributions. Our research addresses the challenge of achieving robust calibration without compromising model performance.
          </p>
          <p>
            We introduce a novel <strong>distribution recalibration</strong> methodology, applicable to any pre-trained predictive model. This technique reframes recalibration as a low-dimensional density estimation problem. Specifically, an auxiliary recalibration model is trained to map features of the base model's forecasts (e.g., its quantiles or parametric representations) to a calibrated predictive distribution. This process targets <em>distribution calibration</em>, a comprehensive form of calibration ensuring that the entire predictive distribution, not merely specific quantiles or moments, is well-calibrated.
          </p>
          <p>
            Our theoretical analysis establishes the correctness of this procedure, contingent on accurate low-dimensional density estimation, and provides uniform convergence bounds. Empirical evaluations demonstrate significant improvements in calibration for both linear and deep Bayesian models across various benchmarks. The proposed framework offers a practical and effective means to enhance the reliability of probabilistic forecasts, advocating for the broader adoption of calibration techniques in machine learning.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Conceptual Demo: Distribution Calibration in Action</h2>
        <div class="content">
          <p>This concise demo illustrates how pre-trained distribution calibrators can be applied to both discrete (classification) and continuous (regression) model outputs. It conceptualizes the usage of calibrators similar to those in the <code>DistCal</code> library, which would first be trained on a dedicated calibration dataset.</p>
          <pre><code class="language-python">
import numpy as np

# --- Mock Uncalibrated Predictions from a Base Model ---
# For a new test point:
# Discrete (Classification): uncalibrated class probabilities (e.g., 3 classes)
uncal_discrete_probs = np.array([[0.65, 0.25, 0.10]])
# Continuous (Regression): uncalibrated quantiles (e.g., 0.1, 0.5, 0.9 levels)
uncal_continuous_quantiles = np.array([[1.8, 2.9, 4.2]])

# --- Placeholder Pre-Trained Calibrator Functions ---
# These simulate applying calibrators from the 'DistCal' library
# after they have been trained.

def apply_discrete_calibrator(uncal_probs):
    """Simulates applying a trained discrete distribution calibrator."""
    print(f"Input discrete probabilities: {uncal_probs}")
    # Example recalibration: slightly adjust and re-normalize
    # A real calibrator learns a more complex transformation.
    adjusted_probs = uncal_probs + np.array([[0.02, -0.01, -0.01]]) # Small learned shift
    calibrated_probs = np.clip(adjusted_probs, 0.001, 0.999) # Ensure valid probs
    calibrated_probs = calibrated_probs / np.sum(calibrated_probs, axis=1, keepdims=True)
    return calibrated_probs

def apply_continuous_calibrator(uncal_quantiles):
    """Simulates applying a trained continuous distribution calibrator."""
    print(f"Input continuous quantiles: {uncal_quantiles}")
    # Example recalibration: adjust median and spread
    # A real calibrator learns this based on phi(F) and tau.
    # Here, phi(F) are the uncal_quantiles themselves.
    # Let's say it learns to shift the median (middle quantile) slightly
    # and adjust the outer quantiles to change the spread.
    calibrated_q = uncal_quantiles.copy()
    calibrated_q[0, 1] += 0.05 # Shift median
    calibrated_q[0, 0] -= 0.1  # Expand lower tail
    calibrated_q[0, 2] += 0.1  # Expand upper tail
    return calibrated_q

# --- Applying Discrete Calibration ---
calibrated_discrete = apply_discrete_calibrator(uncal_discrete_probs)
print(f"Calibrated discrete probabilities: {np.round(calibrated_discrete, 3)}\n")

# --- Applying Continuous Calibration ---
calibrated_continuous = apply_continuous_calibrator(uncal_continuous_quantiles)
print(f"Calibrated continuous quantiles: {np.round(calibrated_continuous, 3)}")

# Note: The actual 'DistCal' library (using torchuq) provides classes like
# 'DiscreteDistCalibrator' and 'DistCalibrator'. These would be instantiated,
# then trained on calibration data (e.g., discrete_cal.train(cal_probs, cal_labels)),
# before being called on new data (e.g., calibrated_output = discrete_cal(new_probs)).
# This demo focuses on the conceptual application post-training.
          </code></pre>
          <p>This example shows how, after training, a discrete calibrator adjusts class probabilities, and a continuous calibrator adjusts predicted quantiles to provide more reliable uncertainty estimates.</p>
        </div>
      </div>
    </div>
  </div>
</section>
<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@InProceedings{pmlr-v162-kuleshov22a,
  title     = {Calibrated and Sharp Uncertainties in Deep Learning via Density Estimation},
  author    = {Kuleshov, Volodymyr and Deshpande, Shachi},
  booktitle = {Proceedings of the 39th International Conference on Machine Learning},
  pages     = {11683--11693},
  year      = {2022},
  editor    = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Sabato, Sivan and Niu, Gang and Szepesvari, Csaba},
  volume    = {162},
  series    = {Proceedings of Machine Learning Research},
  month     = {17--23 Jul},
  publisher = {PMLR},
  pdf       = {https://proceedings.mlr.press/v162/kuleshov22a/kuleshov22a.pdf},
  url       = {https://proceedings.mlr.press/v162/kuleshov22a.html}
}</code></pre>
    </div>
</section>
<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>