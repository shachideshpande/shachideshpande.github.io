<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="Exploring Distribution Calibration for deep learning models to achieve accurate and sharp probabilistic predictions.">
  <meta property="og:title" content="Distribution Calibration: Accurate & Sharp Uncertainties in Deep Learning"/>
  <meta property="og:description" content="A method for improving the reliability of probabilistic predictions from machine learning models through distribution recalibration."/>
  <meta property="og:url" content="https://shachideshpande.github.io/blog-distribution-calibration/"/> <meta property="og:image" content="static/image/your_banner_image.png" /> <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <meta name="twitter:title" content="Distribution Calibration: Accurate & Sharp Uncertainties">
  <meta name="twitter:description" content="Improving reliability of ML model predictions with distribution recalibration.">
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png"> <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords" content="Distribution Calibration, Deep Learning, Uncertainty Quantification, Machine Learning, Probabilistic Predictions, Recalibration, Density Estimation, ICML 2022">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Distribution Calibration</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Calibrated and Sharp Uncertainties in Deep Learning via Density Estimation</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://www.cs.cornell.edu/~kuleshov/" target="_blank">Volodymyr Kuleshov</a>,</span>
              <span class="author-block">
                <a href="https://shachideshpande.github.io/" target="_blank">Shachi Deshpande</a>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">ICML 2022</span>
              </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                   <span class="link-block">
                  <a href="https://proceedings.mlr.press/v162/kuleshov22a/kuleshov22a.pdf" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper (PMLR)</span>
                </a>
              </span>

              <span class="link-block">
                <a href="https://arxiv.org/abs/2112.07184" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="ai ai-arxiv"></i>
                </span>
                <span>arXiv</span>
              </a>
            </span>

            <span class="link-block">
              <a href="https://github.com/shachideshpande/DistCal" target="_blank" class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                <i class="fab fa-github"></i>
              </span>
              <span>Code</span>
            </a>
          </span>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>
</section>

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Accurate probabilistic predictions can be characterized by two properties calibration and sharpness. However, standard maximum likelihood training yields models that are poorly calibrated and thus inaccurate a 90% confidence interval typically does not contain the true outcome 90% of the time. This paper argues that calibration is important in practice and is easy to maintain by performing low-dimensional density estimation. We introduce a simple training procedure based on recalibration that yields calibrated models without sacrificing overall performance; unlike previous approaches, ours ensures the most general property of distribution calibration and applies to any model, including neural networks. We formally prove the correctness of our procedure assuming that we can estimate densities in low dimensions and we establish uniform convergence bounds. Our results yield empirical performance improvements on linear and deep Bayesian models and suggest that calibration should be increasingly leveraged across machine learning.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">The Essence of Distribution Recalibration</h2>
        <div class="content has-text-justified">
          <p>
            The predictive uncertainty of machine learning models, particularly deep neural networks, is often miscalibrated. For instance, a 90% prediction interval from a standard maximum likelihood-trained model may not contain the true outcome with the claimed 90% frequency. This discrepancy between nominal and empirical coverage poses risks in safety-critical applications where reliable uncertainty quantification is paramount.
          </p>
          <p>
            Effective probabilistic forecasting necessitates two fundamental properties: <strong>calibration</strong>, ensuring that predicted probabilities align with observed outcome frequencies, and <strong>sharpness</strong>, indicating the concentration of predictive distributions. Our research addresses the challenge of achieving robust calibration without compromising model performance.
          </p>
          <p>
            We introduce a novel <strong>distribution recalibration</strong> methodology, applicable to any pre-trained predictive model. This technique reframes recalibration as a low-dimensional density estimation problem. Specifically, an auxiliary recalibration model is trained to map features of the base model's forecasts (e.g., its quantiles or parametric representations) to a calibrated predictive distribution. This process targets <em>distribution calibration</em>, a comprehensive form of calibration ensuring that the entire predictive distribution, not merely specific quantiles or moments, is well-calibrated.
          </p>
          <p>
            Our theoretical analysis establishes the correctness of this procedure, contingent on accurate low-dimensional density estimation, and provides uniform convergence bounds. Empirical evaluations demonstrate significant improvements in calibration for both linear and deep Bayesian models across various benchmarks. The proposed framework offers a practical and effective means to enhance the reliability of probabilistic forecasts, advocating for the broader adoption of calibration techniques in machine learning.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Conceptual Demo: Distribution Calibration in Action</h2>
        <div class="content">
          <p>This concise demo illustrates how pre-trained distribution calibrators can be applied to both discrete (classification) and continuous (regression) model outputs. It conceptualizes the usage of calibrators similar to those in the <code>DistCal</code> library, which would first be trained on a dedicated calibration dataset.</p>
          <pre><code