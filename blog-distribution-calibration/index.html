<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="A blog post on Distribution Calibration for deep learning models, ensuring accurate and sharp probabilistic predictions.">
  <meta property="og:title" content="Distribution Calibration: Accurate & Sharp Uncertainties in Deep Learning"/>
  <meta property="og:description" content="Learn how distribution recalibration can improve the reliability of probabilistic predictions from machine learning models."/>
  <meta property="og:url" content="https://shachideshpande.github.io/blog-distribution-calibration/"/>
  <meta property="og:image" content="static/image/your_banner_image.png" /> <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <meta name="twitter:title" content="Distribution Calibration: Accurate & Sharp Uncertainties">
  <meta name="twitter:description" content="Improving reliability of ML model predictions with distribution recalibration.">
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png"> <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords" content="Distribution Calibration, Deep Learning, Uncertainty Quantification, Machine Learning, Probabilistic Predictions, Recalibration, Density Estimation">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Distribution Calibration Blog</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Calibrated and Sharp Uncertainties in Deep Learning via Density Estimation</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://www.cs.cornell.edu/~kuleshov/" target="_blank">Volodymyr Kuleshov</a><sup>*</sup>,</span>
              <span class="author-block">
                <a href="https://shachideshpande.github.io/" target="_blank">Shachi Deshpande</a><sup>*</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">Cornell University<br>Proceedings of the 39<sup>th</sup> International Conference on Machine Learning (ICML 2022)</span>
              <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                   <span class="link-block">
                  <a href="https://proceedings.mlr.press/v162/kuleshov22a/kuleshov22a.pdf" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper (PMLR)</span>
                </a>
              </span>

              <span class="link-block">
                <a href="https://arxiv.org/abs/2112.07184" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="ai ai-arxiv"></i>
                </span>
                <span>arXiv</span>
              </a>
            </span>

            <span class="link-block">
              <a href="https://proceedings.mlr.press/v162/kuleshov22a.html" target="_blank"
              class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                <i class="fas fa-book"></i>
              </span>
              <span>PMLR Page</span>
            </a>
            </span>

            <span class="link-block">
              <a href="https://github.com/shachideshpande/DistCal" target="_blank" class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                <i class="fab fa-github"></i>
              </span>
              <span>Code</span>
            </a>
          </span>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <source src="static/videos/banner_video.mp4" type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Brief visual explanation or highlight of the method. (Update this video and text)
      </h2>
    </div>
  </div>
</section>
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Blog Post: Reliable AI Predictions with Distribution Calibration</h2>
        <div class="content has-text-justified">
          <p>
            In the world of Artificial Intelligence, especially with complex deep learning models, getting a prediction is one thing, but knowing how much to trust that prediction is another. Standard machine learning models often give us probabilistic outputs â€“ for example, a 90% confidence that an image contains a cat. However, these models can be poorly calibrated: that 90% confidence might not actually mean there's a 90% chance of the prediction being correct.
          </p>
          <p>
            Why is this a problem? Imagine an AI diagnosing medical conditions or guiding autonomous vehicles. If the model says it's 95% sure, but in reality, it's only correct 70% of the time, the consequences can be serious. For truly accurate and reliable probabilistic predictions, we need two key properties: <strong>calibration</strong> (where the model's stated confidence levels match its actual likelihood of being correct) and <strong>sharpness</strong> (where the confidence intervals are as narrow or precise as possible).
          </p>
          <p>
            Our work, "Calibrated and Sharp Uncertainties in Deep Learning via Density Estimation," introduces a straightforward yet powerful solution: <strong>distribution recalibration</strong>. This method adjusts the predictions of any pre-trained model, including sophisticated neural networks, to make them well-calibrated without hurting their overall performance. The core idea is to treat recalibration as a low-dimensional density estimation task. We learn an auxiliary "recalibration" model that takes the original model's forecast (perhaps represented by a few key features like its quantiles) and maps it to a new, calibrated probability distribution.
          </p>
          <p>
            This approach ensures a strong form of calibration called *distribution calibration*, meaning the entire shape of the predicted probability distribution is corrected, not just specific confidence points. We've shown theoretically and empirically that this technique leads to more trustworthy and accurate uncertainty estimates. This is a significant step towards building AI systems that we can confidently rely on in critical applications.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Conceptual Code Demo: The Recalibration Idea</h2>
        <div class="content">
          <p>The following Python-like pseudocode illustrates the core concept of the recalibration framework. It's a simplified view, but it captures the essence of training a recalibrator model `R` to adjust the outputs of your primary model `H`.</p>
          <pre><code class="language-python">
# Conceptual Demo: Distribution Recalibration

# Assume you have:
# H_model: Your pre-trained primary model (e.g., a neural network)
# calibration_data: A separate dataset of (inputs, true_outputs) for training the recalibrator
# featurize_forecast: A function to extract key features (e.g., quantiles) from H_model's output
# Recalibrator_R: A model to learn the calibration mapping (e.g., a small neural net or density estimator)

# --- Step 1: Generate features from H_model's forecasts on calibration data ---
forecast_features_for_R_training = []
actual_outcomes_for_R_training = []

for x_cal, y_cal in calibration_data:
    raw_forecast = H_model.predict(x_cal)  # Get raw prediction/distribution from H_model
    features_F = featurize_forecast(raw_forecast) # Extract features from this raw forecast
    forecast_features_for_R_training.append(features_F)
    actual_outcomes_for_R_training.append(y_cal)

# --- Step 2: Train the Recalibrator_R model ---
# Recalibrator_R learns to map the observed forecast_features to the actual_outcomes.
# This is typically done by minimizing a proper scoring rule (e.g., log-loss for classification
# or a quantile-based loss for regression) to ensure R outputs a calibrated distribution.

Recalibrator_R.train(forecast_features_for_R_training, actual_outcomes_for_R_training)

# --- Step 3: Make Calibrated Predictions on new, unseen data ---
def get_calibrated_prediction(new_input_data):
    raw_forecast_new = H_model.predict(new_input_data)
    features_F_new = featurize_forecast(raw_forecast_new)
    
    # Recalibrator_R takes the features of the raw forecast and outputs
    # parameters for a *new, calibrated* predictive distribution.
    calibrated_forecast_parameters = Recalibrator_R.predict(features_F_new)
    
    # From these calibrated_forecast_parameters, you can derive mean,
    # accurately calibrated confidence intervals, the full predictive density, etc.
    return calibrated_forecast_parameters

# --- Example Usage ---
# unseen_data_point = get_some_new_data()
# calibrated_prediction_distribution = get_calibrated_prediction(unseen_data_point)
# Now you can use 'calibrated_prediction_distribution' for decision-making
# with more reliable uncertainty estimates.
          </code></pre>
          <p>This demo highlights that recalibration happens as a post-processing step, using a dedicated calibration dataset to learn how to correct the initial model's probabilistic outputs.</p>
        </div>
      </div>
    </div>
  </div>
</section>
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <img src="static/images/carousel1.jpg" alt="Illustration of miscalibrated vs calibrated predictions"/> <h2 class="subtitle has-text-centered">
          Visualizing calibration: before and after. (Update description)
        </h2>
      </div>
      <div class="item">
        <img src="static/images/carousel2.jpg" alt="Example application of calibrated uncertainties"/> <h2 class="subtitle has-text-centered">
          Impact in a real-world scenario. (Update description)
        </h2>
      </div>
      <div class="item">
        <img src="static/images/carousel3.jpg" alt="Sharpness vs Calibration trade-off example"/> <h2 class="subtitle has-text-centered">
         Achieving both sharpness and calibration. (Update description)
       </h2>
     </div>
     <div class="item">
      <img src="static/images/carousel4.jpg" alt="Further result or plot"/> <h2 class="subtitle has-text-centered">
        Another key result or visualization. (Update description)
      </h2>
    </div>
  </div>
</div>
</div>
</section>
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video Presentation (Placeholder)</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
          <p>Link to your ICML talk or a dedicated video explaining the work will go here.</p>
        </div>
      </div>
    </div>
  </div>
</section>
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster (Placeholder)</h2>
      <iframe  src="static/pdfs/sample_poster.pdf" width="100%" height="550">
      </iframe>
      <p class="has-text-centered">Link to your ICML poster PDF will go here.</p>
    </div>
  </div>
</section>
<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@InProceedings{pmlr-v162-kuleshov22a,
  title     = {Calibrated and Sharp Uncertainties in Deep Learning via Density Estimation},
  author    = {Kuleshov, Volodymyr and Deshpande, Shachi},
  booktitle = {Proceedings of the 39th International Conference on Machine Learning},
  pages     = {11683--11693},
  year      = {2022},
  editor    = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Sabato, Sivan and Niu, Gang and Szepesvari, Csaba},
  volume    = {162},
  series    = {Proceedings of Machine Learning Research},
  month     = {17--23 Jul},
  publisher = {PMLR},
  pdf       = {https://proceedings.mlr.press/v162/kuleshov22a/kuleshov22a.pdf},
  url       = {https://proceedings.mlr.press/v162/kuleshov22a.html}
}</code></pre>
    </div>
</section>
<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from theÂ <a href="https://nerfies.github.io" target="_blank">Nerfies</a>Â project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>